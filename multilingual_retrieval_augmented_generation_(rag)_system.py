# -*- coding: utf-8 -*-
"""Multilingual_Retrieval_Augmented_Generation_(RAG)_System.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11M_IL4UoHAIS6YmASPE28FLlQTQ7tNki
"""

!pip install -U langchain langchain-community transformers accelerate sentence-transformers faiss-cpu pypdf pymupdf

!apt install poppler-utils -y
!apt install tesseract-ocr -y
!apt install tesseract-ocr-ben -y  # for Bangla OCR
!pip install pytesseract pdf2image

import os
import pytesseract
from pdf2image import convert_from_path
from google.colab import drive


drive.mount('/content/drive')

def extract_text_ocr(pdf_path, lang='ben+eng', save_path=None):
    # If saved OCR exists
    if save_path and os.path.exists(save_path):
        print(f"Loading saved OCR output from {save_path}")
        with open(save_path, 'r', encoding='utf-8') as f:
            return f.read()

    print("Performing OCR on PDF pages...")
    pages = convert_from_path(pdf_path)
    full_text = ""
    for i, img in enumerate(pages):
        text = pytesseract.image_to_string(img, lang=lang)
        full_text += text + "\n"

    # Save the OCR output if save_path is provided
    if save_path:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(full_text)
        print(f"OCR output saved to {save_path}")

    return full_text


pdf_path = "/content/drive/MyDrive/10MS_TASK/HSC26-Bangla1st-Paper.pdf"
ocr_txt_path = "/content/drive/MyDrive/10MS_TASK/HSC26-Bangla1st-Paper-OCR.txt"

# load OCR text
extracted_text = extract_text_ocr(pdf_path, lang='ben+eng', save_path=ocr_txt_path)

print("üìÑ Extracted OCR Text (preview first 3000 chars):\n")
print(extracted_text[:3000])

import re
from sentence_transformers import SentenceTransformer
import faiss
from transformers import pipeline
import torch


ocr_txt_path = "/content/drive/MyDrive/10MS_TASK/HSC26-Bangla1st-Paper-OCR.txt"

# Load and clean OCR text
with open(ocr_txt_path, 'r', encoding='utf-8') as f:
    corpus_text = f.read()

# Basic cleaning (normalize newlines and spaces)
corpus_text = re.sub(r'\n+', '\n', corpus_text)
corpus_text = re.sub(r'\s+', ' ', corpus_text)
corpus_text = corpus_text.strip()

# Chunk the text into overlapping chunks
chunk_size = 100
overlap = 20
words = corpus_text.split()
chunks = []
start = 0
while start < len(words):
    end = min(start + chunk_size, len(words))
    chunk = " ".join(words[start:end])
    chunks.append(chunk)
    start += chunk_size - overlap

print(f"Total chunks created: {len(chunks)}")

# Load embedding model
embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')

# Embed chunks
chunk_embeddings = embed_model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)

# Build FAISS index
dim = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(chunk_embeddings)

# Load QA pipeline
device = 0 if torch.cuda.is_available() else -1
qa_pipeline = pipeline(
    "question-answering",
    model="deepset/xlm-roberta-large-squad2",
    tokenizer="deepset/xlm-roberta-large-squad2",
    device=device
)

# Short-term memory to store last Q&A
short_term_memory = []

def retrieve(query, top_k=5):
    query_emb = embed_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_emb, top_k)
    retrieved_chunks = [chunks[i] for i in indices[0]]
    return retrieved_chunks

def generate_answer(question, context):
    result = qa_pipeline(question=question, context=context)
    return result.get('answer', '')

def ask(question):
    # last 3 Q&A for short-term memory context
    memory_context = " ".join([f"Q: {q} A: {a}" for q, a in short_term_memory[-3:]])
    retrieved = retrieve(question, top_k=5)
    combined_context = memory_context + " " + " ".join(retrieved)
    answer = generate_answer(question, combined_context)
    short_term_memory.append((question, answer))
    if len(short_term_memory) > 10:
        short_term_memory.pop(0)
    return answer

print("System ready. Type your questions (English or Bangla). Type 'exit' or 'quit' to stop.")

while True:
    user_question = input("User Question: ")
    if user_question.strip().lower() in ['exit', 'quit']:
        break
    answer = ask(user_question)
    print("Answer:", answer)

from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# ground-truth evaluation set
evaluation_set = [
    {
        "question": "‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡ßÅ‡¶™‡ßÅ‡¶∞‡ßÅ‡¶∑ ‡¶ï‡¶æ‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
        "answer": "‡¶∂‡ßÅ‡¶Æ‡ßç‡¶≠‡ßÅ‡¶®‡¶æ‡¶•"
    },
    {
        "question": "‡¶ï‡¶æ‡¶ï‡ßá ‡¶Ö‡¶®‡ßÅ‡¶™‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶æ‡¶ó‡ßç‡¶Ø ‡¶¶‡ßá‡¶¨‡¶§‡¶æ ‡¶¨‡¶≤‡ßá ‡¶â‡¶≤‡ßç‡¶≤‡ßá‡¶ñ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá?",
        "answer": "‡¶Æ‡¶æ‡¶Æ‡¶æ‡¶ï‡ßá"
    },
    {
        "question": "‡¶¨‡¶ø‡¶Ø‡¶º‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶Ø‡¶º ‡¶ï‡¶≤‡ßç‡¶Ø‡¶æ‡¶£‡ßÄ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶Ø‡¶º‡¶∏ ‡¶ï‡¶§ ‡¶õ‡¶ø‡¶≤",
        "answer": "‡¶™‡¶®‡ßá‡¶∞‡ßã"
    }
]

def evaluate_rag_system(eval_set):
    similarities = []
    for item in eval_set:
        question = item["question"]
        true_answer = item["answer"]

        # system-generated answer
        generated_answer = ask(question)

        # Embed both answers
        true_vec = embed_model.encode([true_answer], convert_to_numpy=True)
        pred_vec = embed_model.encode([generated_answer], convert_to_numpy=True)

        # cosine similarity
        score = cosine_similarity(true_vec, pred_vec)[0][0]
        similarities.append(score)

        print(f"Q: {question}")
        print(f"GT: {true_answer}")
        print(f"Pred: {generated_answer}")
        print(f"Cosine Similarity: {score:.4f}\n")

    avg_score = np.mean(similarities)
    print(f"üîç Average Cosine Similarity across {len(eval_set)} samples: {avg_score:.4f}")


evaluate_rag_system(evaluation_set)